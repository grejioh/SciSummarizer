class Summary {
    title string @description("论文的英文标题")
    chinese_title string @description("论文的中文标题，必要术语（如Transformer）不需要翻译")
    repo string?  @description("文章的代码仓库地址")
    // core_ideas_summary string  @description("文章解析，根据论文摘要部分，概括论文核心内容 归纳主要观点 字数控制在150-200字左右 保证精炼而全面")
    core_ideas_summary string  @description("文章解析，根据论文摘要部分，翻译文章摘要，归纳主要观点, 保证精炼而全面")
    innovations string[] @description("创新点，每条一句话")
    methodology string[] @description("研究方法，每条一句话")
    conclusions string[] @description("研究结论，每条一句话")
}

function SummaryPaper(paperContent: string) -> Summary {
    client AFallbackClient
    prompt #"
        基于这里提供的论文内容，输出内容，使用中文，必要术语（如Transformer）不需要翻译:
        {{paperContent}}

        {{ ctx.output_format }}
    "#
}

test AQuantumPaper {
    functions [SummaryPaper]
    args {
        paperContent #"
            MultiscaleVisionTransformerWithDeepClustering-GuidedRefinementforWeaklySupervisedObjectLocalizationDavidMinkwanKimDepartmentofComputerScienceHanyangUniversitySeoul,RepublicofKoreaSinhaeChaByeongkeunKangDepartmentofElectronicEngineeringSeoulNationalUniversityofScienceandTechnologySeoul,RepublicofKoreaAbstract—Thisworkaddressesthetaskofweakly-supervisedobjectlocalization.Thegoalistolearnobjectlocalizationusingonlyimage-levelclasslabels,whicharemucheasiertoobtaincomparedtoboundingboxannotations.Thistaskisimportantbecauseitreducestheneedforlabor-intensiveground-truthannotations.However,methodsforobjectlocalizationtrainedusingweaksupervisionoftensufferfromlimitedaccuracyinlocalization.Toaddressthischallengeandenhancelocalizationaccuracy, we propose a multiscale object localization transformer(MOLT).Itcomprisesmultipleobjectlocalizationtransformersthatextractpatchembeddingsacrossvariousscales.Moreover,weintroduceadeepclustering-guidedrefinementmethodthatfurtherenhanceslocalizationaccuracybyutilizingseparatelyextractedimagesegments.Thesesegmentsareobtainedbyclusteringpixelsusingconvolutionalneuralnetworks.Finally,wedemonstratetheeffectivenessofourproposedmethodbyconductingexperimentsonthepubliclyavailableILSVRC-2012dataset.IndexTerms—weakly-supervisedobjectlocalization,weakly-supervisedlearning,visiontransformer,neuralnetworksI.INTRODUCTIONRecently,therehasbeenconsiderableinterestinweakly-supervisedobjectlocalization(WSOL)techniquesduetotheirabilitytoeffectivelyreducetheneedforlabor-intensivehumanannotationsintrainingneuralnetworksforobjectlocalization [1]–[4]. Unlike supervised learning-based methodsthatdependontrainingimages,image-levelclasslabels,andboundingboxes,WSOLmethodssolelyutilizeimagesandimage-levelclasslabels[3].Asannotatingboundingboxesrequiressubstantiallymoreeffortthanannotatingimage-levelclasslabels,WSOLmethodsofferapromisingsolutiontominimizehumanannotationefforts.Therefore,wefocusoninvestigatingWSOLmethods.Oquabetal.introducedoneoftheearliestapproachesforWSOL in [1], which employed convolutional neural networks(CNNs)alongwithaslidingwindowandmax-pooling.Toavoidtheslidingwindowforimprovedefficiency,Zhouetal.usedglobalaveragepoolinginsteadofmax-poolingandThisworkwassupportedinpartbytheNationalResearchFoundationofKorea(NRF)grantfundedbytheKoreagovernment(MSIT)(No.RS-2023-00252434).Correspondingauthor:ByeongkeunKang(byeongkeun.kang@seoultech.ac.kr).generated a dense localization map which is referred to as theclassactivationmap(CAM)[2].However,duetothelackofsupervisionforlocalizationduringtraining,theseapproachestend to localize only the most discriminative part of an objectandstrugglewithaccuratelyidentifyingtheentireobjectregion.Therefore,inthesubsequenteffort,researchershaveexplored various techniques to localize the entire object regionratherthanjustthemostdiscriminativepart.Thesemethodsincludeusingpseudolabels[5],adversarialerasing[6],mul-tiplefeaturemaps[7],andalternativearchitectures[3].Considering alternative architectures, Gaoet al.introduced avision transformer [8]-based method to leverage self-attentionmechanismsforlong-rangedependenciesratherthanlocalreceptivefieldsinconvolutionallayers[3].In this paper, we also investigate a vision transformer-basedframeworkthattakesadvantageofself-attentionmechanismstocapturelong-rangedependenciesforlocalizinganobject’sentireregion.Giventhatpreviousmethodsstillhavelimi-tationsinaccuratelylocalizingtheentireobjectregion,weproposeamultiscaleobjectlocalizationtransformer(MOLT)thatextractspatchembeddingsusingvariousreceptivefields.Then,byaggregatingtheoutputpatchembeddingsfromthetransformers,themultiscaletransformereffectivelylocalizesobject components, ranging from coarse discriminative regionstofinedetails.Additionally,weintroduceadeepclustering-guided refinement method to enhance object localization evenfurther.Thismethodinvolvesrefiningpixel-levelactivationvaluesusingtheactivationvaluesofthecorrespondingimagesegment.Thesesegmentsareobtainedbyclusteringpixelsusingconvolutionalneuralnetworks.Insummary,weproposeaWSOLframeworkthatconsistsofamultiscaleobjectlocalizationtransformer,deeppixelclustering,andadeepclustering-guidedrefinementmethod,asillustratedinFig.1.Theproposedmultiscaletransformercomprisesthreeobjectlocalizationtransformersandistolocalizeanobject’sentirelocationratherthanonlythemostdiscriminativepartbyactivatingtheobjectinvaryinggran-ularities.Thedeepclustering-guidedrefinementistofur-therimprovelocalizationaccuracyusingseparatelyextractedimagesegments.Finally,weverifytheeffectivenessofthearXiv:2312.09584v1  [cs.CV]  15 Dec 2023
Fig.1.Illustrationoftheproposedframework.Theproposedframeworkconsistsofamultiscaleobjectlocalizationtransformer,deeppixelclustering,anddeepclustering-guidedrefinement.Themultiscaletransformercomprisesthreeobjectlocalizationtransformers.Therefinementmethodtakesthecombinedclassactivationmapfromthemultiscaletransformerandimagesegmentsfromdeeppixelclustering,andpredictstheobjectclassandlocation.proposedmethodbymeasuringTop-1,Top-5,andGT-knownlocalizationaccuraciesusingtheILSVRC-2012dataset[9].II.PROPOSED METHODWeproposeamultiscaleobjectlocalizationtransformerthatpredictsboththeobject’sclassandlocationinaninputimage.Furthermore,weintroduceadeepclustering-guidedrefinementmethodthatenhanceslocalizationaccuracybyrefiningactivationvaluesatthepixellevel.A.MultiscaleObjectLocalizationTransformerThe multiscale object localization transformer (MOLT) em-ploysavisiontransformerarchitecture[8]topredictboththeobjectclassanditslocationgivenaninputimage.Toaccuratelypredictthem,wefirstconstructamultiscaleimagepyramidbyresizingtheimageintothreedifferentresolutions(H1×H1,H2×H2,H3×H3),asshowninFig.1.Thisistoexplicitlyextractembeddingsatvaryinggranularities.Subsequently,eachimageinthepyramidisfedintothecorrespondingobjectlocalizationtransformer,whichisbasedonthearchitecturepresentedin[3].Giventhei-thimageIiinthepyramid,itisfirstcroppedintoNi×Nipatches, where each patch has a size ofˆHi×ˆHi.Thesepatchesarethenlinearlyprojectedusinglearnableparameters to obtain the patch embeddingˆz0i∈RNi2×D. ThepatchembeddingisthenappendedafteraclasstokenwhichisaD-dimensionalvector,resultinginthetokenembedding ̃z0i∈RLi×D,whereLi=Ni2+1.Finally,theinputtokenembeddingz0i∈RLi×Disobtainedbyadding ̃z0iwiththelearnablepositionembeddingandbyapplyinglayernormalization[10].Theinputtokenembeddingz0iisthenprocessedbythetransformer encoder, which comprises consecutive transformerblocks.Inthe(b+1)-thblock,theinputembeddingzbiundergoes a sequence of processing steps: layer normalization,multi-headself-attention(MSA)witharesidualconnection,and a multi-layer perceptron (MLP) with a residual connection.TheMLPconsistsoftwolayersandemploystheGaussianerrorlinearunits(GELU)function[11]forintermediatenon-linearity.Theoutputzb+1i∈RLi×DfromtheMLPservesastheinputembeddingforthesubsequentblock.The multi-head self-attention (MSA) consists ofMattentionheadsandemploysquery,key,andvalue-basedself-attention.Thenormalizedinputsequence ̄zbiisfirstdividedintoMvectors,eachwithdimensionsofLi×DK.Then,forthem-thhead,queryqbi,m,keykbi,m,andvaluevbi,mvectorsareobtained by linearly projecting them-th vector from ̄zbiusinglearnableparameters.AnattentionmatrixAbi,m∈RLi×Liisthencalculatedbymultiplyingthequeryvectorqbi,mwiththetransposedkeyvectorkbi,m,byscalingtheresultusingapredeterminedscalar,andbyapplyingasoftmaxfunction.The output vector for the head is computed by multiplying theattentionmatrixAi,mwiththevaluevectorvbi,m.Theoutputof the MSA is obtained by concatenating the outputs from theself-attentionheadsandbyapplyingafullyconnectedlayer.Asmentionedearlier,weemploytheMSAwitharesidualconnection,sotheoutputisaddedtotheinputsequencezbi.Theoutputembeddingsequencefromthetransformeren-coder is reshaped into a tensor with dimensions ofNi×Ni×D,after excluding the output class token. Subsequently, the classscoremapSi∈RNi×Ni×Cisobtainedbyapplyinga2Dconvolution layer to the reshaped tensor, whereCdenotes thenumberofclasses.Finally,theclassscoresiiscomputedbyapplyingglobalaveragepoolingtotheclassscoremapSi.Duringinference,theargmaxfunctionisappliedtotheclassscoresitopredicttheobjectclasslabel.Topredictanobject’slocation,aclassactivationmap(CAM)isestimatedusingtheattentionmapsAbi,mandtheclassscoremapSi.Firstly,theattentionmapsAbi,mareaveragedoverthemultipleheads(m)andsummedoverthetransformer blocks (b), resulting in an averaged attention map ̄Ai∈RLi×Li.Then,theforegroundmapMfgiisestimatedusing the attention weights connecting from theNi×Nipatchtokens to the class token. Finally, the CAMMcamiis obtainedby multiplying the foreground mapMfgiwith the class scoremapSi.The CAMsMcamifrom the object localization transformersarethenupsampledtomatchthehighestresolutionamongthem.Finally,eachofthemapsisnormalizedusingmin-maxnormalization,andthenormalizedmapsareusedtogeneratethecombinedCAMMccam.Duringinference,anobject’s

        "#
    }
}