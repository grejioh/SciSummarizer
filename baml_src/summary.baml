class Summary {
    title string @description("论文的英文标题")
    chinese_title string @description("论文的中文标题")
    repo string?  @description("文章的代码仓库地址")
    core_ideas_summary string  @description("文章解析，概括论文核心内容 归纳主要观点 字数控制在150-200字左右 保证精炼而全面")
    innovations string[] @description("创新点，每条一句话")
    methodology string[] @description("研究方法，每条一句话")
    conclusions string[] @description("研究结论，每条一句话")
}

function SummaryPaper(paperContent: string) -> Summary {
    client AFallbackClient
    prompt #"
        基于这里提供的论文内容，输出内容，使用中文:
        {{paperContent}}

        {{ ctx.output_format }}
    "#
}

test AQuantumPaper {
    functions [SummaryPaper]
    args {
        paperContent #"
            Universal Organizer of SAM for Unsupervised Semantic Segmentation
Tingting Li, Gensheng Pei, Xinhao Cai, Qiong Wang, Huafeng Liu and Yazhou Yao
School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China {litingting, peigsh, xinhao, wangq, liu.hua.feng, yazhou.yao}@njust.edu.cn
Abstract—Unsupervised semantic segmentation (USS) aims to achieve high-quality segmentation without manual pixel-level annotations. Existing USS models provide coarse category classifi- cation for regions, but the results often have blurry and imprecise edges. Recently, a robust framework called the segment anything model (SAM) has been proven to deliver precise boundary object masks. Therefore, this paper proposes a universal organizer based on SAM, termed as UO-SAM, to enhance the mask quality of USS models. Specifically, using only the original image and the masks generated by the USS model, we extract visual features to obtain positional prompts for target objects. Then, we activate a local region optimizer that performs segmentation using SAM on a per-object basis. Finally, we employ a global region optimizer to incorporate global image information and refine the masks to obtain the final fine-grained masks. Compared to existing methods, our UO-SAM achieves state-of-the-art performance. Our codes are available at https://github.com/NUST-Machine- Intelligence-Laboratory/UO-SAM.
Index Terms—Unsupervised Semantic Segmentation, Segment Anything Model, Universal Optimizer
I. INTRODUCTION
Semantic segmentation [1]–[4] is a critical task in computer vision that aims to assign semantic labels [5], [6] to each pixel in an image. It has been widely applied in various fields such as medical imaging [7], aerial remote sensing [8], and autonomous driving [9]. In recent years, the success of deep learning techniques and the availability of large-scale pixel-level annotated datasets have greatly improved semantic segmentation performance. However, annotating images at the pixel level requires significant human resources, making it extremely costly. This has led to the emergence of weakly supervised [4], [10]–[16] and unsupervised [17]–[19] semantic segmentation (USS). This paper focuses on USS, which aims to capture pixel-level semantics from unlabeled data and is one of the most challenging tasks.
Early USS works [23]–[26] attempt to train models to learn semantic consistency without prior knowledge. For ex- ample, DeepCluster [27] introduces clustering to group fea- tures, IIC [28] matches pairs of categories and maximizes the mutual information between them during training, and PiCIE [29] utilizes geometric consistency as an inductive bias by clustering pixel-level features with invariance and equal variances. Given the good semantic consistency showcased by pre-training self-supervised frameworks, self-supervised ViT models (e.g., DINO [30]) have gained considerable popularity. TransFGU [31] uses class activation maps [32] as pixel-level pseudo-labels, mapping high-level semantic classes discovered
 
Fig. 1. State-of-the-art comparison on three datasets (i.e., ImageNet-S50 [20], PASCAL VOC [21], and COCO-Stuff [22]). We present UO-SAM, which, for the first time, outperforms the top-specialized baselines on multiple USS benchmarks.
in DINO to low-level pixel features. STEGO [33] designs new loss functions to make same-class features more com- pact while preserving relationships among different categories. HP [34] proposes hidden positive factors to maintain local semantic consistency and designs gradient propagation strate- gies to match them. SmooSeg [35] simplifies segmentation using smoothness priors and introduces a smoothness loss to promote intra-segment smoothness while maintaining dis- continuity between different segments. CAUSE [36] leverages insights from causal reasoning and proposes a causal unsuper- vised semantic analysis framework. LUSS [20] demonstrates the feasibility of large-scale USS tasks and constructs a new dataset, ImageNet-S, along with its baseline.
Unsupervised semantic segmentation presents numerous challenges [37], and the quality of masks produced by existing models remains subpar due to issues such as incomplete segmentation, incorrect segmentation, and blurred boundaries. Consequently, we propose a universal organizer framework predicated on SAM [38], termed as UO-SAM, designed to enhance the precision and quality of masks. Specifically, we introduce the local region optimizer (LRO) to rectify unclear mask boundaries. This module isolates specific object-related feature fragments from image features utilizing mask features, subsequently establishing a confidence map that mirrors the position of the target regions. We amalgamate the reliable position prompts with SAM optimization to generate masks
LRO
,score ,score ,score
GRO
Fig. 2. The architecture of our UO-SAM. We introduce a universal organizer of segment anything model (UO-SAM). It consists of the local region optimizer (LRO) and the global region optimizer (GRO) modules to improve the USS baseline performance.
with precise boundaries. Additionally, to ensure the complete- ness of target objects in multi-label scenarios, we propose the global region optimizer (GRO). This module harnesses contextual information from the image, focuses on all target objects, and engineers a semantic voting mechanism to better integrate the predictions from the LRO module. This results in a reduction of erroneous pixels in the masks and yields enhanced segmentation results.
As shown in Fig. 1, we present extensive experimental evaluations conducted on multiple USS datasets. Building upon established unsupervised semantic segmentation mod- els like PASS [20], CAUSE [36], and SmooSeg [35], UO- SAM delivers remarkable achievements across diverse set- tings. Notably, our UO-SAM surpasses existing state-of-the- art approaches. Specifically, on ImageNet-S50 [20], our ap- proach exhibits superior performance compared to PASS with a 6.6% improvement. UO-SAM improves CAUSE-MLP and CAUSE-TR by 2.2% and 1.5% on PASCAL VOC [21]. Our approach outperforms SmooSeg, CAUSE-MLP and CAUSE- TR on COCO-Stuff [22] with performance improvements of 0.4%, 1.1%, and 0.6%, respectively. Our contributions can be summarized as follows:
(1) We propose a universal organizer framework based on SAM (UO-SAM), which consists of two modules: the local region optimizer (LRO, see §II-B) and the global region optimizer (GRO, see §II-C).
(2) We introduce LRO to alleviate the issues of blurry and unclear mask boundaries along object edges. Moreover, we present GRO to ensure the accuracy and completeness of over- all segmentation, thereby minimizing incomplete segmentation instances.
(3) Our UO-SAM improves the segmentation accuracy of multiple baseline models by a sizable amount and requires no additionalre-trainingorfine-tuning.
II. PROPOSED METHOD
In this section, we will introduce our UO-SAM. We begin by providing an overview of the framework in §II-A. Then, in §II-B, we present the local region optimizer module for opti- mizing object segmentation by leveraging foreground position prompts. Additionally, we discuss the global region optimizer module in §II-C.
A. Overall Framework
In this work, we present the universal organizer of segment anything model (UO-SAM) for the USS task. As illustrated in Fig. 2, our approach integrates two primary components: the local region optimizer (LRO) and the global region optimizer (GRO). Following the structure of SAM [38], our UO-SAM is constructed around three key elements: an image encoder (EncI ), a prompt encoder (EncP ), and a lightweight mask decoder (DecM ).
Upon processing an image through the image encoder, an image embedding is generated which can be effectively inter- rogated by various input prompts to yield object masks. Within the LRO component, the location confidence map (LCM) and the largest connected component (LCC) are employed to identify point and box prompts. Consequently, optimized masks for target objects are acquired through cascaded post- processing. Subsequently, the GRO module utilizes a semantic voting mechanism to execute full image optimization on it, thereby generating the final mask.
B. Local Region Optimizer
The user only needs to provide an image I and the mask M generated by an unsupervised semantic segmentation model. UO-SAM autonomously localizes the positional information of the target object and feeds it into the prompt encoder in the form of points, bounding boxes, or other visual prompts. Given the image I and the mask M, we apply the image encoder of SAM to extract their visual features, represented as:
FI =EncI(F),MI =EncI(M), (1)

        "#
    }
}